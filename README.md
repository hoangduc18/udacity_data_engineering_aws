# Data Engineering Projects

Welcome to my **Data Engineering Projects** repository! This collection showcases projects from three comprehensive data engineering courses, each focusing on essential skills and tools for building scalable data solutions. Explore these projects to see real-world applications of data pipelines, cloud data warehousing, and data lake solutions. There are total 5 courses but here i only showcase the last 3 courses.

## Courses Overview

### 1. Automate Data Pipelines

In the **Automate Data Pipelines** course, I learned to design, build, and manage data pipelines using **Apache Airflow**. This course emphasized automating ETL processes to streamline data workflows and ensure reliability. Key skills covered include:

- Setting up and managing **Airflow DAGs** to automate task workflows
- Building reusable and dynamic pipeline components
- Implementing data quality checks to maintain data integrity
- Using Airflow operators and hooks to interact with AWS and other platforms

**Project**: Created a data pipeline for a music streaming company that automates ETL tasks, moving data from **AWS S3** to **Amazon Redshift** using Airflow. The pipeline enables seamless data flow, monitoring, and automated error handling for reliable data processing.

### 2. Cloud Data Warehouse

The **Cloud Data Warehouse** course focused on designing and building scalable data warehouses using **Amazon Redshift**. I learned to optimize data models for analytical queries and perform ETL processes to efficiently populate the warehouse. Key concepts include:

- Cloud data warehousing principles and best practices
- Designing **star schemas** for optimized analytical workloads
- Implementing ETL pipelines to load data into Redshift
- Optimizing Redshift queries for large-scale datasets

**Project**: Developed a cloud data warehouse for a music streaming app, storing song and user activity data in Redshift. The warehouse is designed to support complex analytical queries, enabling efficient data analysis and reporting.

### 3. Spark and Data Lakes

In the **Spark and Data Lakes** course, I explored big data processing using **Apache Spark** and data lake architecture on **AWS S3**. This course emphasized handling large datasets and performing distributed data transformations. Key learning points include:

- Understanding the structure and function of data lakes
- Using **Spark** for scalable data transformations and analytics
- Storing and managing data in AWS S3 as a data lake
- Integrating data lake storage with Spark for machine learning applications

**Project**: Built a data lakehouse solution for STEDI, a human balance analytics company. The solution processes sensor data using Spark and AWS Glue, preparing data for machine learning models that detect balance steps.

---

## Repository Structure

Each folder in this repository corresponds to one of the courses above, containing:
- Code for the projectâ€™s ETL pipeline
- Detailed explanations of the project setup and design
- Key scripts for data processing and transformations

## Technologies Used

- **Apache Airflow** for workflow automation
- **Amazon Redshift** for cloud data warehousing
- **AWS S3** for data storage in a data lake
- **Apache Spark** for distributed data processing
- **Python** for scripting ETL processes and data transformations

## License

This repository is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

Each course and project demonstrates critical data engineering skills, equipping me to manage data at scale effectively. Explore each project directory for more details and code implementations!
